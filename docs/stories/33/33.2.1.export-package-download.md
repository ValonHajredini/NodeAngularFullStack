# Story 33.2.1: Export Package Download

**Epic:** 33.2 Export Package Distribution (18 pts) **Story Points:** 6 **Priority:** High
**Status:** Approved **Created:** 2025-10-24

---

## Description

Implement secure package download endpoint to allow users to retrieve completed export packages
(.tar.gz archives). Use streaming for efficient large file transfers, validate user permissions,
track download metrics, and provide proper HTTP headers for browser download behavior.

---

## Acceptance Criteria

### Download API Endpoint

- [ ] Create `GET /api/tool-registry/export-jobs/:jobId/download` endpoint
- [ ] Endpoint requires JWT authentication
- [ ] Endpoint streams .tar.gz file to client (not buffered in memory)
- [ ] Returns HTTP 200 OK with file stream on success
- [ ] Returns HTTP 404 if job not found or package not ready
- [ ] Returns HTTP 403 if user unauthorized to download
- [ ] Returns HTTP 410 Gone if package expired/deleted

### Permission Checks

- [ ] User must be job creator OR admin to download package
- [ ] Validate export job exists and status is 'completed'
- [ ] Validate package file exists on filesystem
- [ ] Return detailed error if permission denied (who can download)

### HTTP Headers

- [ ] Set `Content-Type: application/gzip` header
- [ ] Set `Content-Disposition: attachment; filename="export-{toolName}-{timestamp}.tar.gz"` header
- [ ] Set `Content-Length` header with file size
- [ ] Set `Cache-Control: private, max-age=3600` (1 hour cache)
- [ ] Set `X-Package-Size` custom header with human-readable size (e.g., "12.5 MB")

### File Streaming

- [ ] Use Node.js `fs.createReadStream()` for efficient streaming
- [ ] Stream in chunks (no full file read into memory)
- [ ] Support range requests (HTTP 206 Partial Content) for resume capability
- [ ] Handle stream errors gracefully (file not found, read errors)
- [ ] Set appropriate read buffer size for performance

### Download Tracking

- [ ] Update `export_jobs` table with `download_count` column (increment)
- [ ] Update `export_jobs` table with `last_downloaded_at` timestamp
- [ ] Log download events with userId, jobId, and file size
- [ ] Track bandwidth usage (optional: for monitoring)

### Package Cleanup

- [ ] Delete package file after 30 days (configurable retention)
- [ ] Mark old jobs for cleanup via background job
- [ ] Keep database record but set `package_path` to null
- [ ] Allow re-export if package deleted (user can re-generate)

### Error Handling

- [ ] Handle file not found errors (404)
- [ ] Handle permission errors (403)
- [ ] Handle stream errors (500)
- [ ] Log all errors with context (jobId, userId, file path)
- [ ] Return user-friendly error messages

### Testing

- [ ] Integration tests for download endpoint (≥85% coverage)
- [ ] Test download by job creator (200 OK)
- [ ] Test download by admin (200 OK)
- [ ] Test download by unauthorized user (403 Forbidden)
- [ ] Test download of non-existent package (404 Not Found)
- [ ] Test stream errors handled gracefully
- [ ] Test large file download (1GB+)
- [ ] Test concurrent downloads don't conflict

---

## Tasks

### 1. Add Download Columns to Export Jobs Table

**Estimated:** 1 hour **Dependencies:** Story 33.1.2 (Export Jobs Schema) **Description:** Add
download tracking columns to database schema.

**Subtasks:**

1. Create migration `028_add_download_tracking_to_export_jobs.sql`
2. Add `download_count` INTEGER DEFAULT 0 column
3. Add `last_downloaded_at` TIMESTAMP column (nullable)
4. Add `package_expires_at` TIMESTAMP column (nullable)
5. Add `package_retention_days` INTEGER DEFAULT 30 column
6. Run migration on dev database
7. Update ExportJob interface in shared types
8. Rebuild shared package
9. Update ExportJobRepository with new fields
10. Test migration rollback (DOWN script)

### 2. Create Package Download Endpoint

**Estimated:** 3 hours **Dependencies:** Task 1 **Description:** Implement download API endpoint
with authentication.

**Subtasks:**

1. Add route `GET /export-jobs/:jobId/download` to export routes
2. Create `downloadPackage()` method in ExportController
3. Validate jobId format (UUID)
4. Retrieve export job from ExportJobRepository
5. Check job status is 'completed'
6. Verify package_path is not null
7. Check file exists on filesystem
8. Validate user permission (creator or admin)
9. Stream file to response
10. Update download_count and last_downloaded_at

### 3. Implement Permission Validation

**Estimated:** 2 hours **Dependencies:** Task 2 **Description:** Verify user authorized to download
package.

**Subtasks:**

1. Extract userId from JWT token (req.user.userId)
2. Compare userId with job.userId (job creator check)
3. Check if user is admin (req.user.role === 'admin')
4. Return 403 Forbidden if not creator and not admin
5. Include error message: "Only job creator or admin can download"
6. Log unauthorized download attempts
7. Add unit tests for permission checks
8. Test admin can download any job's package
9. Test user can only download own packages
10. Test anonymous user cannot download (401)

### 4. Implement File Streaming

**Estimated:** 3 hours **Dependencies:** Task 2 **Description:** Stream .tar.gz file efficiently
without buffering.

**Subtasks:**

1. Import `fs.createReadStream()` from Node.js fs module
2. Get file path from job.package_path
3. Check file exists using `fs.stat()` (get file size)
4. Create read stream with `highWaterMark: 64 * 1024` (64KB chunks)
5. Pipe stream to response object
6. Handle stream 'error' event (log and return 500)
7. Handle stream 'end' event (log success)
8. Set response status to 200 OK
9. Add unit tests for streaming logic
10. Test with large files (1GB+)

### 5. Set HTTP Response Headers

**Estimated:** 1 hour **Dependencies:** Task 4 **Description:** Configure headers for proper browser
download behavior.

**Subtasks:**

1. Set `Content-Type: application/gzip` header
2. Generate filename: `export-${toolName}-${timestamp}.tar.gz`
3. Set `Content-Disposition: attachment; filename="${filename}"` header
4. Set `Content-Length` header from file size
5. Set `Cache-Control: private, max-age=3600` header
6. Add custom `X-Package-Size` header with human-readable size
7. Test headers in browser (file downloads with correct name)
8. Test Content-Length matches actual file size
9. Verify browser shows download progress bar
10. Document header usage in JSDoc

### 6. Implement Range Request Support

**Estimated:** 3 hours **Dependencies:** Task 4 **Description:** Support HTTP range requests for
resume capability.

**Subtasks:**

1. Check if `Range` header present in request
2. Parse range header (e.g., "bytes=0-1023")
3. Extract start and end byte positions
4. Validate range is within file size
5. Create read stream with `start` and `end` options
6. Set response status to 206 Partial Content
7. Set `Content-Range` header (e.g., "bytes 0-1023/10240")
8. Set `Accept-Ranges: bytes` header
9. Test resume capability (download → pause → resume)
10. Add integration test for range requests

### 7. Update Download Tracking

**Estimated:** 2 hours **Dependencies:** Task 2 **Description:** Track download count and timestamp
in database.

**Subtasks:**

1. Increment `download_count` column after successful download
2. Update `last_downloaded_at` column with current timestamp
3. Use UPDATE query with WHERE clause (job_id = $1)
4. Handle concurrent downloads (atomic increment)
5. Log download event with userId, jobId, file size
6. Add `getDownloadStats(jobId)` method to repository
7. Test download_count increments correctly
8. Test last_downloaded_at updates on each download
9. Test concurrent downloads don't lose counts
10. Add unit tests for download tracking

### 8. Implement Package Expiration

**Estimated:** 2 hours **Dependencies:** Task 1 **Description:** Automatically expire old packages
for cleanup.

**Subtasks:**

1. Set `package_expires_at` when export completes
2. Calculate expiration: `completed_at + retention_days`
3. Add `isPackageExpired()` method to ExportJobRepository
4. Check expiration before allowing download
5. Return 410 Gone if package expired
6. Create cleanup job to delete expired packages
7. Delete package file from filesystem on expiration
8. Set `package_path` to null after deletion
9. Keep job record for history (don't delete)
10. Add unit tests for expiration logic

### 9. Create Package Cleanup Background Job

**Estimated:** 3 hours **Dependencies:** Task 8 **Description:** Automated job to clean up old
export packages.

**Subtasks:**

1. Create `apps/api/src/jobs/package-cleanup.job.ts`
2. Query export_jobs with `package_expires_at < NOW()`
3. Loop through expired jobs
4. Delete package file from filesystem (`fs.unlink()`)
5. Update job record: `package_path = null`, `package_size_bytes = null`
6. Log cleanup operations (count of deleted packages, freed space)
7. Schedule job to run daily at 3 AM (cron job)
8. Add error handling for file deletion failures
9. Track total disk space freed
10. Add unit tests for cleanup logic

### 10. Add Download Metrics Logging

**Estimated:** 1 hour **Dependencies:** Task 7 **Description:** Log download events for monitoring
and analytics.

**Subtasks:**

1. Log download start with userId, jobId, file size
2. Log download complete with duration
3. Calculate bandwidth used (file size / duration)
4. Use structured logging (JSON format)
5. Include correlation ID (request ID)
6. Log to separate download metrics file (optional)
7. Add metrics to monitoring dashboard (optional)
8. Track failed downloads (stream errors)
9. Calculate average download time for file size ranges
10. Document metrics in monitoring docs

### 11. Integration Tests for Download Endpoint

**Estimated:** 4 hours **Dependencies:** Task 2 **Description:** Comprehensive tests for download
functionality.

**Subtasks:**

1. Create `apps/api/tests/integration/export-download.test.ts`
2. Setup test database with completed export job
3. Create test package file in `/tmp/test-packages/`
4. Test download by job creator (200 OK, file streamed)
5. Test download by admin (200 OK)
6. Test download by unauthorized user (403 Forbidden)
7. Test download of non-existent job (404 Not Found)
8. Test download of job without package (404)
9. Test download_count increments
10. Test last_downloaded_at updates

### 12. Large File Download Tests

**Estimated:** 2 hours **Dependencies:** Task 11 **Description:** Test performance with large export
packages.

**Subtasks:**

1. Create test package file (1GB size)
2. Test download completes successfully
3. Measure download time (should be < 2 minutes)
4. Verify streaming (memory usage should be constant)
5. Test concurrent downloads of large files
6. Monitor CPU usage during download
7. Test range requests with large files
8. Test pause/resume with large files
9. Verify Content-Length header matches file size
10. Document performance benchmarks

### 13. Frontend Integration: Download Button

**Estimated:** 3 hours **Dependencies:** Task 2 **Description:** Add download button to export
progress modal (Story 32.2.4).

**Subtasks:**

1. Add "Download Package" button to ExportProgressModalComponent
2. Button appears when job status is 'completed'
3. Button disabled if package expired (show tooltip)
4. Call ExportJobService.downloadPackage(jobId) on click
5. Use window.open() or fetch() with blob download
6. Show download progress (if possible with fetch API)
7. Handle download errors (file not found, permission denied)
8. Show success toast notification after download
9. Add unit tests for download button logic
10. Update Story 32.2.4 dev notes with download integration

### 14. API Documentation

**Estimated:** 1 hour **Dependencies:** Task 2 **Description:** Document download endpoint in
Swagger/OpenAPI.

**Subtasks:**

1. Add JSDoc comments with Swagger annotations to controller
2. Document GET /export-jobs/:jobId/download endpoint
3. Add request parameters (jobId in path)
4. Add authentication requirement (Bearer token)
5. Document response headers (Content-Type, Content-Disposition, etc.)
6. Add response examples (200 OK, 403 Forbidden, 404 Not Found, 410 Gone)
7. Document range request support (206 Partial Content)
8. Add example curl command for downloading
9. Generate Swagger JSON specification
10. Verify endpoint appears in /api-docs UI

### 15. Error Handling and Edge Cases

**Estimated:** 2 hours **Dependencies:** Task 2 **Description:** Handle all error scenarios
gracefully.

**Subtasks:**

1. Test stream error (file deleted during download)
2. Handle ENOENT error (file not found)
3. Handle EACCES error (permission denied)
4. Handle disk full error during download
5. Test network interruption (client closes connection)
6. Test concurrent download of same file
7. Test download during package cleanup (race condition)
8. Return user-friendly error messages
9. Log all errors with full context
10. Add error recovery strategies where possible

---

## Dev Notes

### Download API Endpoint Specification

```
GET /api/tool-registry/export-jobs/:jobId/download
Authorization: Bearer <jwt_token>

Request Headers:
- Authorization: Bearer <token>
- Range: bytes=0-1023 (optional, for resume)

Response 200 OK:
Headers:
  Content-Type: application/gzip
  Content-Disposition: attachment; filename="export-customer-form-2025-10-24.tar.gz"
  Content-Length: 12582912 (12 MB)
  Cache-Control: private, max-age=3600
  Accept-Ranges: bytes
  X-Package-Size: 12 MB

Body: <binary file stream>

Response 206 Partial Content (with Range request):
Headers:
  Content-Type: application/gzip
  Content-Range: bytes 0-1023/12582912
  Content-Length: 1024
  Accept-Ranges: bytes

Body: <partial binary stream>

Response 403 Forbidden:
{
  "status": "error",
  "message": "Only job creator or admin can download this package",
  "code": "DOWNLOAD_UNAUTHORIZED",
  "timestamp": "2025-10-24T15:30:00Z"
}

Response 404 Not Found:
{
  "status": "error",
  "message": "Export package not found or not ready",
  "code": "PACKAGE_NOT_FOUND",
  "timestamp": "2025-10-24T15:30:00Z"
}

Response 410 Gone:
{
  "status": "error",
  "message": "Export package has expired and was deleted",
  "code": "PACKAGE_EXPIRED",
  "timestamp": "2025-10-24T15:30:00Z",
  "expiresAt": "2025-09-24T00:00:00Z"
}
```

### Download Controller Implementation

```typescript
// apps/api/src/controllers/export.controller.ts (addition)
import { Request, Response, NextFunction } from 'express';
import * as fs from 'fs';
import * as path from 'path';

/**
 * Download completed export package
 *
 * @route GET /api/tool-registry/export-jobs/:jobId/download
 * @param req - Express request with jobId in params
 * @param res - Express response (file stream)
 * @param next - Express next function
 * @returns 200 OK with file stream
 *
 * @swagger
 * /api/tool-registry/export-jobs/{jobId}/download:
 *   get:
 *     summary: Download completed export package
 *     tags: [Export]
 *     security:
 *       - bearerAuth: []
 *     parameters:
 *       - in: path
 *         name: jobId
 *         required: true
 *         schema:
 *           type: string
 *           format: uuid
 *         description: Export job ID
 *       - in: header
 *         name: Range
 *         schema:
 *           type: string
 *         description: HTTP range header for resume capability (e.g., "bytes=0-1023")
 *     responses:
 *       200:
 *         description: Export package downloaded successfully
 *         content:
 *           application/gzip:
 *             schema:
 *               type: string
 *               format: binary
 *       206:
 *         description: Partial content (range request)
 *       403:
 *         description: User unauthorized to download
 *       404:
 *         description: Package not found or not ready
 *       410:
 *         description: Package expired and deleted
 */
async downloadPackage(req: Request, res: Response, next: NextFunction): Promise<void> {
  try {
    const { jobId } = req.params;
    const userId = req.user!.userId;
    const isAdmin = req.user!.role === 'admin';

    // Step 1: Retrieve export job
    const job = await this.exportJobRepo.findById(jobId);
    if (!job) {
      res.status(404).json({
        status: 'error',
        message: 'Export job not found',
        code: 'JOB_NOT_FOUND',
        timestamp: new Date().toISOString(),
      });
      return;
    }

    // Step 2: Check job status
    if (job.status !== 'completed') {
      res.status(404).json({
        status: 'error',
        message: 'Export package not ready. Job status: ' + job.status,
        code: 'PACKAGE_NOT_READY',
        timestamp: new Date().toISOString(),
      });
      return;
    }

    // Step 3: Check package exists
    if (!job.packagePath) {
      res.status(410).json({
        status: 'error',
        message: 'Export package has expired and was deleted',
        code: 'PACKAGE_EXPIRED',
        timestamp: new Date().toISOString(),
        expiresAt: job.packageExpiresAt,
      });
      return;
    }

    // Step 4: Verify permission
    if (job.userId !== userId && !isAdmin) {
      res.status(403).json({
        status: 'error',
        message: 'Only job creator or admin can download this package',
        code: 'DOWNLOAD_UNAUTHORIZED',
        timestamp: new Date().toISOString(),
      });
      return;
    }

    // Step 5: Check file exists on filesystem
    const filePath = job.packagePath;
    let fileStats: fs.Stats;
    try {
      fileStats = await fs.promises.stat(filePath);
    } catch (error) {
      res.status(404).json({
        status: 'error',
        message: 'Export package file not found on server',
        code: 'FILE_NOT_FOUND',
        timestamp: new Date().toISOString(),
      });
      return;
    }

    // Step 6: Generate filename
    const toolName = job.toolData?.toolName || 'export';
    const timestamp = new Date(job.completedAt!).toISOString().split('T')[0];
    const filename = `export-${toolName.replace(/\s+/g, '-').toLowerCase()}-${timestamp}.tar.gz`;

    // Step 7: Handle range requests (resume capability)
    const range = req.headers.range;
    if (range) {
      const parts = range.replace(/bytes=/, '').split('-');
      const start = parseInt(parts[0], 10);
      const end = parts[1] ? parseInt(parts[1], 10) : fileStats.size - 1;
      const chunkSize = end - start + 1;

      // Set 206 Partial Content headers
      res.status(206);
      res.set('Content-Range', `bytes ${start}-${end}/${fileStats.size}`);
      res.set('Content-Length', chunkSize.toString());
      res.set('Content-Type', 'application/gzip');
      res.set('Content-Disposition', `attachment; filename="${filename}"`);
      res.set('Accept-Ranges', 'bytes');

      // Create read stream with range
      const stream = fs.createReadStream(filePath, { start, end });
      stream.pipe(res);

      stream.on('error', (error) => {
        console.error('Stream error during download:', error);
        if (!res.headersSent) {
          res.status(500).json({ status: 'error', message: 'Download failed' });
        }
      });

      return;
    }

    // Step 8: Set response headers for full download
    res.status(200);
    res.set('Content-Type', 'application/gzip');
    res.set('Content-Disposition', `attachment; filename="${filename}"`);
    res.set('Content-Length', fileStats.size.toString());
    res.set('Cache-Control', 'private, max-age=3600');
    res.set('Accept-Ranges', 'bytes');
    res.set('X-Package-Size', this.formatFileSize(fileStats.size));

    // Step 9: Stream file
    const stream = fs.createReadStream(filePath, {
      highWaterMark: 64 * 1024, // 64KB chunks
    });

    stream.pipe(res);

    stream.on('error', (error) => {
      console.error('Stream error during download:', error);
      if (!res.headersSent) {
        res.status(500).json({ status: 'error', message: 'Download failed' });
      }
    });

    stream.on('end', async () => {
      // Step 10: Update download tracking
      await this.exportJobRepo.update(jobId, {
        downloadCount: job.downloadCount + 1,
        lastDownloadedAt: new Date(),
      });

      console.log(`Package downloaded: jobId=${jobId}, userId=${userId}, size=${fileStats.size}`);
    });
  } catch (error) {
    next(error);
  }
}

/**
 * Format file size to human-readable string
 */
private formatFileSize(bytes: number): string {
  if (bytes < 1024) return `${bytes} B`;
  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;
}
```

**Why This Implementation:**

- **Streaming:** Uses `fs.createReadStream()` for memory-efficient downloads
- **Range Support:** Handles HTTP range requests for pause/resume
- **Permission Checks:** Verifies user is creator or admin
- **Error Handling:** Checks file exists, handles stream errors
- **Headers:** Sets proper headers for browser download behavior
- **Tracking:** Updates download count and timestamp

### Package Cleanup Job Implementation

```typescript
// apps/api/src/jobs/package-cleanup.job.ts
import { Injectable } from '@nestjs/common';
import { Cron, CronExpression } from '@nestjs/schedule';
import { ExportJobRepository } from '../repositories/export-job.repository';
import * as fs from 'fs/promises';

/**
 * Background job to clean up expired export packages
 * Runs daily at 3 AM
 */
@Injectable()
export class PackageCleanupJob {
  constructor(private readonly exportJobRepo: ExportJobRepository) {}

  @Cron(CronExpression.EVERY_DAY_AT_3AM)
  async cleanupExpiredPackages(): Promise<void> {
    console.log('Starting package cleanup job...');

    try {
      // Find jobs with expired packages
      const expiredJobs = await this.exportJobRepo.findExpiredPackages();

      let deletedCount = 0;
      let freedSpace = 0;

      for (const job of expiredJobs) {
        if (!job.packagePath) continue;

        try {
          // Get file size before deletion
          const stats = await fs.stat(job.packagePath);
          const fileSize = stats.size;

          // Delete package file
          await fs.unlink(job.packagePath);

          // Update job record
          await this.exportJobRepo.update(job.jobId, {
            packagePath: null,
            packageSizeBytes: null,
          });

          deletedCount++;
          freedSpace += fileSize;

          console.log(`Deleted expired package: jobId=${job.jobId}, size=${fileSize}`);
        } catch (error) {
          console.error(`Failed to delete package: jobId=${job.jobId}, error=${error.message}`);
        }
      }

      console.log(
        `Package cleanup complete: ${deletedCount} packages deleted, ${this.formatFileSize(
          freedSpace
        )} freed`
      );
    } catch (error) {
      console.error('Package cleanup job failed:', error);
    }
  }

  private formatFileSize(bytes: number): string {
    if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
    if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
    return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;
  }
}
```

**Why Cron Job:**

- Runs automatically at 3 AM (low traffic time)
- Deletes expired packages (free disk space)
- Keeps database records for history (package_path = null)
- Logs cleanup operations for monitoring

---

## Testing

### Integration Test Example

```typescript
// apps/api/tests/integration/export-download.test.ts
import request from 'supertest';
import app from '../../src/app';
import * as fs from 'fs/promises';
import * as path from 'path';

describe('Export Package Download', () => {
  let adminToken: string;
  let userToken: string;
  let jobId: string;
  let packagePath: string;

  beforeAll(async () => {
    // Get tokens
    adminToken = await getTestJWT('admin@example.com', 'Admin123!@#');
    userToken = await getTestJWT('user@example.com', 'User123!@#');

    // Create test package
    jobId = 'download-test-job-123';
    packagePath = path.join('/tmp/test-packages', `${jobId}.tar.gz`);
    await fs.mkdir(path.dirname(packagePath), { recursive: true });
    await fs.writeFile(packagePath, 'test package content', 'utf-8');

    // Create job record
    await createTestExportJob(jobId, packagePath);
  });

  afterAll(async () => {
    await fs.unlink(packagePath).catch(() => {});
  });

  it('should download package by job creator', async () => {
    const response = await request(app)
      .get(`/api/tool-registry/export-jobs/${jobId}/download`)
      .set('Authorization', `Bearer ${userToken}`)
      .expect(200);

    expect(response.headers['content-type']).toBe('application/gzip');
    expect(response.headers['content-disposition']).toContain('attachment');
    expect(response.headers['content-disposition']).toContain('.tar.gz');
    expect(response.body).toBeDefined();
  });

  it('should return 403 for unauthorized user', async () => {
    const otherUserToken = await getTestJWT('other@example.com', 'Other123!@#');

    await request(app)
      .get(`/api/tool-registry/export-jobs/${jobId}/download`)
      .set('Authorization', `Bearer ${otherUserToken}`)
      .expect(403);
  });

  it('should support range requests', async () => {
    const response = await request(app)
      .get(`/api/tool-registry/export-jobs/${jobId}/download`)
      .set('Authorization', `Bearer ${adminToken}`)
      .set('Range', 'bytes=0-99')
      .expect(206);

    expect(response.headers['content-range']).toMatch(/bytes 0-99/);
    expect(response.headers['content-length']).toBe('100');
  });
});
```

---

## Dependencies

### Blocked By:

- Story 33.1.1: Export Orchestrator Service (generates packages)
- Story 33.1.2: Export Jobs Database Schema (stores package_path)
- Story 33.1.3: Export Job Status Tracking (job status API)

### Blocks:

- Story 33.2.2: Package Verification & Security (checksum validation)
- Story 33.2.3: Export History & Re-download (download history UI)

### Related:

- Story 32.2.4: Export Progress Modal (download button integration)

---

## QA Gate

**Gate File:** `docs/qa/gates/33.2.1-export-package-download.yml`

### Quality Criteria (Weighted):

| Criterion                 | Weight | Target                  | Validation Method |
| ------------------------- | ------ | ----------------------- | ----------------- |
| Integration Test Coverage | 30%    | ≥85% endpoint tested    | Jest coverage     |
| Permission Checks         | 25%    | RBAC enforced           | Security tests    |
| Streaming Performance     | 20%    | Large files (1GB+) work | Performance tests |
| Error Handling            | 15%    | All error paths tested  | Manual testing    |
| API Documentation         | 10%    | Swagger docs complete   | Swagger UI        |

**Minimum Score:** 90/100 to pass gate

---

## Notes

### ★ Insight ─────────────────────────────────────

**Why Streaming:**

- **Memory Efficiency:** Streaming processes file in chunks (no full file in memory)
- **Large File Support:** Can stream multi-GB files without memory issues
- **Resume Capability:** Range requests enable pause/resume downloads

**Permission Strategy:**

- **Job Creator:** User who initiated export can download
- **Admin Bypass:** Admins can download any package (full access)
- **No Public Access:** Export packages are private (authentication required)

**Cleanup Strategy:**

- **30-Day Retention:** Balance between storage costs and user convenience
- **Keep Records:** Database records preserved for audit trail
- **Re-export Option:** Users can re-generate expired packages

─────────────────────────────────────────────────

**Story State:** Draft **Last Updated:** 2025-10-24 **Next Review:** After implementation completion
